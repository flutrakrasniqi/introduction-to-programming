{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#pandas\" data-toc-modified-id=\"pandas-1\">pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-data\" data-toc-modified-id=\"Loading-data-1.1\">Loading data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text\" data-toc-modified-id=\"Text-1.1.1\">Text</a></span></li><li><span><a href=\"#Excel\" data-toc-modified-id=\"Excel-1.1.2\">Excel</a></span></li><li><span><a href=\"#The-internet\" data-toc-modified-id=\"The-internet-1.1.3\">The internet</a></span></li></ul></li><li><span><a href=\"#DataFrames\" data-toc-modified-id=\"DataFrames-1.2\">DataFrames</a></span><ul class=\"toc-item\"><li><span><a href=\"#Indexing\" data-toc-modified-id=\"Indexing-1.2.1\">Indexing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Columns\" data-toc-modified-id=\"Columns-1.2.1.1\">Columns</a></span></li><li><span><a href=\"#Rows\" data-toc-modified-id=\"Rows-1.2.1.2\">Rows</a></span></li><li><span><a href=\"#pandas-madness\" data-toc-modified-id=\"pandas-madness-1.2.1.3\">pandas madness</a></span></li></ul></li><li><span><a href=\"#Boolean-indexing\" data-toc-modified-id=\"Boolean-indexing-1.2.2\">Boolean indexing</a></span></li></ul></li><li><span><a href=\"#Summaries\" data-toc-modified-id=\"Summaries-1.3\">Summaries</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "\n",
    "Our next special topic is how to analyze simple data sets in Python.\n",
    "\n",
    "## pandas\n",
    "\n",
    "As you may have noticed by now, the first step for almost any specialist task in Python is to [import](extras/glossary.ipynb#import) an additional [package](extras/glossary.ipynb#package) that provides some extra functionality. The most popular package for loading and working with tables of data is called `pandas`. We met it briefly already in the lesson on files, when we learned about [delimited text data files](files.ipynb#delimited-text).\n",
    "\n",
    "### Loading data\n",
    "\n",
    "#### Text\n",
    "\n",
    "Recall that a delimited text file stores a table of data as plain text, with one particular character (most often the comma) reserved as a [separator](extras/glossary.ipynb#separator) to mark the boundaries between the columns of the table. Here is an example data file in which the separator is a comma (a *csv* file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird,HeartRate,Depth,Duration\n",
      "EP19,88.8,5,1.05\n",
      "EP19,103.4,9,1.1833333\n",
      "EP19,97.4,22,1.9166667\n",
      "EP19,85.3,25.5,3.4666667\n",
      "EP19,60.6,30.5,7.0833333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "filepath = os.path.join('data', 'penguins.csv')\n",
    "\n",
    "with open(filepath) as f:\n",
    "    for linenum in range(6):\n",
    "        print(f.readline(), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file stores a table of data on the [heart rates of penguins during fishing dives](https://doi.org/10.1242/jeb.013235). Each row represents one dive, with the following columns:\n",
    "\n",
    "* *Bird*: An ID code for the penguin making the dive.\n",
    "* *HeartRate*: The penguin's heart rate during the dive, in beats per minute (bpm).\n",
    "* *Depth*: The depth of the dive (in meters).\n",
    "* *Duration*: The duration of the dive (in minutes).\n",
    "\n",
    "Let's import `pandas` and use its `read_csv()` function to read this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Bird  HeartRate  Depth   Duration\n",
      "0      EP19       88.8    5.0   1.050000\n",
      "1      EP19      103.4    9.0   1.183333\n",
      "2      EP19       97.4   22.0   1.916667\n",
      "3      EP19       85.3   25.5   3.466667\n",
      "4      EP19       60.6   30.5   7.083333\n",
      "..      ...        ...    ...        ...\n",
      "120  EP3901       48.4  170.0  11.533333\n",
      "121  EP3901       50.8   37.0   8.216667\n",
      "122  EP3901       49.6  160.0  11.300000\n",
      "123  EP3901       56.4  180.0  10.283333\n",
      "124  EP3901       55.2  170.0  10.366667\n",
      "\n",
      "[125 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "penguins = pandas.read_csv(filepath)\n",
    "\n",
    "print(penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The `...` indicates that some rows have not been shown, in order to keep the printout brief.)\n",
    "\n",
    "#### Excel\n",
    "\n",
    "As well as `read_csv()`, `pandas` provides many functions for reading data from different sources. You may sometimes need to read from a Microsoft Excel spreadsheet file. There is a function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_excel = pandas.read_excel(os.path.join('data', 'penguins.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`read_excel()` can take additional [arguments](extras/glossary.ipynb#argument) for specifying which sheet of the Excel file to read, which rows or columns to skip, etc. in case your spreadsheet is a little messy. As always with new functions, it is a good idea to seek out and read the [online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) to see what the possibilities are and whether there are any important defaults.\n",
    "\n",
    "#### The internet\n",
    "\n",
    "Some `pandas` [IO](extras/glossary.ipynb#io) functions can even read from a file stored online. The first [argument](extras/glossary.ipynb#argument) is the [URL](extras/glossary.ipynb#url) to the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/luketudge/introduction-to-programming/master/content/examples/data/penguins.csv'\n",
    "\n",
    "penguins_from_the_internet = pandas.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "What [type](extras/glossary.ipynb#type) of object do `pandas`' read functions give us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They give us a 'data frame'. A [data frame](extras/glossary.ipynb#dataframe) is very similar to a [matrix](extras/glossary.ipynb#matrix). Like a matrix, a data frame stores values in a 'grid' of rows and columns. But whereas the structure of a matrix may represent all sorts of things, the roles of rows and columns are slightly more specific in a data frame.\n",
    "\n",
    "Each *row* of a data frame represents one 'observation'. An observation is some coherent entity or event for which information was recorded. Each *column* of a data frame represents one measurement or piece of information that was recorded for each of the observations. For example, the observations in a data frame might be people, for whom some physical or demographic measurements were made, or they might be events like purchases in a store, for each of which a customer ID, product, price, etc. were recorded. In the penguins data frame, the observations are dives. (Note that the observations are not penguins, as it might be tempting to think; depth and duration are clearly attributes of individual dives, not of penguins).\n",
    "\n",
    "Each row of a data frame may contain values of heterogeneous [types](extras/glossary.ipynb#type), since not every piece of information recorded about each observation is necessarily of the same kind. For example, the penguins data frame records a [string](extras/glossary.ipynb#string) ID for the bird making the dive, but the other values are [floats](extras/glossary.ipynb#float). The values in a single column *are* necessarily of the same type, since we usually want to compare them with each other. For example, we would not be able to compare the dive depths of different penguins if some depths were recorded as floats, and others as strings such as `'quite deep'` or `'really deep'`.\n",
    "\n",
    "#### Indexing\n",
    "\n",
    "##### Columns\n",
    "\n",
    "We can get individual columns from a `pandas.DataFrame` using the same square parenthesis [indexing](extras/glossary.ipynb#index) as for other Python types. For example, to get a single column, the index is just the name of the column, as a [string](extras/glossary.ipynb#string):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        5.0\n",
      "1        9.0\n",
      "2       22.0\n",
      "3       25.5\n",
      "4       30.5\n",
      "       ...  \n",
      "120    170.0\n",
      "121     37.0\n",
      "122    160.0\n",
      "123    180.0\n",
      "124    170.0\n",
      "Name: Depth, Length: 125, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(penguins['Depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the output above, individual columns have a `dtype` [attribute](extras/glossary.ipynb#attribute) just as `numpy` [arrays](arrays.ipynb#data-types) do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins['Depth'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any column that does not contain only numbers `pandas` will treat as being of data type 'object' (abbreviated to 'O'), which is a sort of catch-all data type for non-numbers. In practice, this usually means [strings](extras/glossary.ipynb#string), as is the case for example for the 'Bird' column in the penguins data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins['Bird'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get multiple columns using a [list](extras/glossary.ipynb#list) of column names. Note the double square parentheses. The outer parentheses are for indexing, and the inner parentheses indicate that the index is a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Depth   Duration\n",
      "0      5.0   1.050000\n",
      "1      9.0   1.183333\n",
      "2     22.0   1.916667\n",
      "3     25.5   3.466667\n",
      "4     30.5   7.083333\n",
      "..     ...        ...\n",
      "120  170.0  11.533333\n",
      "121   37.0   8.216667\n",
      "122  160.0  11.300000\n",
      "123  180.0  10.283333\n",
      "124  170.0  10.366667\n",
      "\n",
      "[125 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(penguins[['Depth', 'Duration']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ever need to check what columns a `pandas.DataFrame` contains, the `columns` attribute can tell us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Bird', 'HeartRate', 'Depth', 'Duration'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rows\n",
    "\n",
    "Whereas indexing columns from a `pandas.DataFrame` is relatively straightforward, indexing rows presents some frustrating subtleties. Let's take a careful look.\n",
    "\n",
    "We can use a [slice](extras/glossary.ipynb#slice) index to get a range of rows, just as we would to get a range of entries from a [list](extras/glossary.ipynb#list). This at least is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Bird  HeartRate  Depth   Duration\n",
      "5   EP19       77.6   32.5   4.766667\n",
      "6   EP19       44.3   38.0   9.133333\n",
      "7   EP19       32.8   32.0  11.000000\n",
      "8   EP19       94.2    6.0   1.316667\n",
      "9   EP19       99.8   10.5   1.483333\n",
      "10  EP19      104.5    6.0   1.166667\n",
      "11  EP19       78.0   19.5   2.716667\n",
      "12  EP19       54.2   27.5   7.250000\n",
      "13  EP19       79.0   33.5   4.783333\n",
      "14  EP19       42.9   40.5  11.866667\n"
     ]
    }
   ],
   "source": [
    "print(penguins[5:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, not all is as it seems here. The first frustration is that we *must* use a slice to index rows like this. We cannot get a single row using a single integer as an index, as we would for a list. And the error message that we get if we try to do this is a horrible mess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/GitHub/introduction-to-programming/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f30fa6d4b5e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpenguins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GitHub/introduction-to-programming/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/introduction-to-programming/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5"
     ]
    }
   ],
   "source": [
    "penguins[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can patch up this limitation by using a slice that contains only one row. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Bird  HeartRate  Depth  Duration\n",
      "5  EP19       77.6   32.5  4.766667\n"
     ]
    }
   ],
   "source": [
    "print(penguins[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pandas madness\n",
    "\n",
    "The astute question to ask here is of course: Why? Why can't we get a single row of a `pandas.DataFrame` just as easily as getting a single entry from a list?\n",
    "\n",
    "The reason (though whether you consider it a *good* reason is for you to decide) is that `pandas` somewhat tries to second-guess what the user wants when they index a data frame. If we use a [slice](extras/glossary.ipynb#slice) index, then `pandas` assumes we want a subset of row numbers, but if we use anything else as the index, such as a single value or a list, then pandas assumes we want a column or subset of columns.\n",
    "\n",
    "So when we asked for `penguins[5]` above, `pandas` actually tried to get a column whose name is the integer `5`. And of course there is no such column in our data frame.\n",
    "\n",
    "###### iloc\n",
    "\n",
    "If we want to index rows and columns, it is best not to let `pandas` second-guess what we are trying to do. Instead, we should tell `pandas` explicitly whether our index is a position or a name. `pandas`' system for doing this is a little unintuitive. A `pandas.DataFrame` has two different [attributes](extras/glossary.ipynb#attribute) for accessing the data by position or by name. These attributes can be indexed.\n",
    "\n",
    "The `iloc` attribute stores the data by position ('`iloc`' is an abbreviation of 'integer location'). So the explicit way to get row number `5` without `5` being interpreted as a column name is to apply the index `[5]` to the `iloc` attribute instead of to the data frame itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bird            EP19\n",
       "HeartRate       77.6\n",
       "Depth           32.5\n",
       "Duration     4.76667\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing attributes also allow for a *row,column* pair of indices, just like a `numpy` [array](extras/glossary.ipynb#array) does. The first index refers to the row, and the second to the column.\n",
    "\n",
    "So for example to get the value in column `2` of row `5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.iloc[5, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is only rarely useful to get columns by position. Columns usually have meaningful names, in which case we should use those instead.\n",
    "\n",
    "###### loc\n",
    "\n",
    "If we want to get rows and columns by name and not by position, the corresponding attribute for this is called `loc` (a not very helpful abbreviation of just 'location').\n",
    "\n",
    "We have seen that columns have names, but do the rows also have names? Indeed they do. The names of the rows are simply the same as their positions. (Though there are some rare cases in which the rows may have been assigned alternative names).\n",
    "\n",
    "So somewhat confusingly, both `iloc` and `loc` give the same result when we just ask for a row, because the integer index is both the position and the name of the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bird            EP19\n",
       "HeartRate       77.6\n",
       "Depth           32.5\n",
       "Duration     4.76667\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.loc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `loc` allows us to additionally index one or more columns by name, which is its real advantage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.loc[5, 'Duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Depth          32.5\n",
       "Duration    4.76667\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.loc[5, ['Depth', 'Duration']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one final important subtlety to note about the `loc` and `iloc` attributes. To see what it is, let's first [assign](extras/glossary.ipynb#assignment) a subset of rows into a new data frame variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Bird  HeartRate  Depth   Duration\n",
      "5   EP19       77.6   32.5   4.766667\n",
      "6   EP19       44.3   38.0   9.133333\n",
      "7   EP19       32.8   32.0  11.000000\n",
      "8   EP19       94.2    6.0   1.316667\n",
      "9   EP19       99.8   10.5   1.483333\n",
      "10  EP19      104.5    6.0   1.166667\n",
      "11  EP19       78.0   19.5   2.716667\n",
      "12  EP19       54.2   27.5   7.250000\n",
      "13  EP19       79.0   33.5   4.783333\n",
      "14  EP19       42.9   40.5  11.866667\n"
     ]
    }
   ],
   "source": [
    "penguins_subset = penguins[5:15]\n",
    "\n",
    "print(penguins_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the printout above, the rows in the new smaller data frame retain their original names. An important consequence of this is that once we have taken a subset of rows, the row names are no longer the same as the row positions. This difference shows up when we use the `iloc` and `loc` indexing methods.\n",
    "\n",
    "Since `iloc` gets rows by position, `iloc[5]` still gets us the sixth row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bird            EP19\n",
       "HeartRate      104.5\n",
       "Depth              6\n",
       "Duration     1.16667\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins_subset.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `loc` gets rows by name, and the names now refer to the positions in the *original* data frame, so `iloc[5]` gets us the row that was the sixth row in the original data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bird            EP19\n",
       "HeartRate       77.6\n",
       "Depth           32.5\n",
       "Duration     4.76667\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins_subset.loc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you read all that correctly:\n",
    "\n",
    "* Indexing a `pandas.DataFrame` does something very different depending on whether the index is a slice or a single value.\n",
    "* In any case, the recommended way of indexing is to index an *attribute* of the data frame rather than the data frame itself.\n",
    "* And to cap it all, these attributes have such fabulously clear and intuitive names as `loc` and `iloc`!\n",
    "\n",
    "If you are now thoroughly confused, you are in good company. Enter a question or complaint about `pandas` indexing into an internet search engine and you will discover a rich literature of lamentations. Indexing is among a handful of irritating minor quirks in the design of the `pandas` package. It probably could have been thought through a little more carefully, but `pandas` is otherwise so useful and has become so widely used that it would now be difficult for the developers to change it and risk breaking all the great data analysis programs that have been written with `pandas` so far. So we are stuck with it.\n",
    "\n",
    "#### Boolean indexing\n",
    "\n",
    "I have perhaps been a little harsh on `pandas`. Its somewhat messy methods for row indexing are not such a big deal if we bear in mind that selecting rows by position is not actually such a common thing to want to do in data analysis. After all, the rows of a data frame are often arranged in an arbitrary order; it is the values stored in the rows that gives them meaning, not their position in the data frame.\n",
    "\n",
    "A much more common task in data analysis is to select rows based on some condition concerning their values. For example, in the penguins data we might want to work with all the rows that represent dives deeper than 150 meters. We would need to get all the rows that have a value greater than `150` in the 'Depth' column, and the position of those rows is irrelevant.\n",
    "\n",
    "In the [lesson on conditions](conditions.ipynb#logic), we learned how to write logical statements to ask whether a certain condition holds. We can use a column of a `pandas.DataFrame` in the same kind of statement. The result is a sequence of [boolean](extras/glossary.ipynb#boolean) values (i.e. `True` or `False`), one for each row in the data frame, stating whether the condition holds for that row.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "120     True\n",
       "121    False\n",
       "122     True\n",
       "123     True\n",
       "124     True\n",
       "Name: Depth, Length: 125, dtype: bool"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins['Depth'] > 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These statements can in turn be used as indices (directly with the data frame, not with `loc` or `iloc`) to select those rows where the statement is `True`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Bird  HeartRate  Depth   Duration\n",
      "32   EP432001       47.8  170.0  10.033333\n",
      "33   EP432001       44.9  160.0   9.983333\n",
      "46   EP432001       48.6  160.0   7.466667\n",
      "47   EP432001       43.8  160.0   8.000000\n",
      "113    EP3901       77.5  225.0   7.466667\n",
      "114    EP3901       71.6  225.0   8.616667\n",
      "117    EP3901       50.6  175.0  10.783333\n",
      "119    EP3901       42.1  165.0  13.533333\n",
      "120    EP3901       48.4  170.0  11.533333\n",
      "122    EP3901       49.6  160.0  11.300000\n",
      "123    EP3901       56.4  180.0  10.283333\n",
      "124    EP3901       55.2  170.0  10.366667\n"
     ]
    }
   ],
   "source": [
    "print(penguins[penguins['Depth'] > 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional [operators](extras/glossary.ipynb#operator) `&` (AND) and `|` (OR) can be used to combine multiple conditions.\n",
    "\n",
    "For example if we want all the rows representing dives that were fairly deep and also quite short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Bird  HeartRate  Depth  Duration\n",
      "21      EP22       97.3   66.5  2.316667\n",
      "38  EP432001       68.1  100.0  3.350000\n"
     ]
    }
   ],
   "source": [
    "print(penguins[(penguins['Depth'] > 50) & (penguins['Duration'] < 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'boolean indexing' is much more commonly useful than indexing by row number.\n",
    "\n",
    "### Summaries\n",
    "\n",
    "There is more good news about `pandas`. A `pandas.DataFrame` has a few really useful [methods](extras/glossary.ipynb#method) for producing summaries of the data.\n",
    "\n",
    "Often one of the first steps when exploring some new data is to get an idea of the spread or 'distribution' of the different values in each column. We might want to know what the lowest and highest values are, what the most common values are, and so on. This gives us a better feel for the scales of measurement, and can sometimes reveal values that are unusual or might even have been entered incorrectly when the data were recorded.\n",
    "\n",
    "#### Numbers\n",
    "\n",
    "Summarizing multiple values in one number is also known as 'aggregating' the values. The `aggregate()` method of a `pandas.DataFrame` [returns](extras/glossary.ipynb#return) a new shorter data frame containing only aggregated values. The first [argument](extras/glossary.ipynb#argument) to `aggregate()` specifies what function or functions to use to aggregate the data. These functions can be [built-in](extras/glossary.ipynb#builtin) Python functions, or functions from a [module](extras/glossary.ipynb#module) that we have [imported](extras/glossary.ipynb#import). A lot of common aggregation functions can be found in the [`statistics` module](https://docs.python.org/3/library/statistics.html) in Python's [standard library](standard_library.ipynb):\n",
    "\n",
    "* `statistics.median()`. The value that is in the 'middle' of the data when the values are arranged in order.\n",
    "* `statistics.mean()`. The sum of all the values, divided by the number of values (i.e. the value that would result if the total amount were 'shared equally' among all the observations).\n",
    "* `statistics.stdev()`. A measure of how far values tend to be from the mean, on average. So this indicates how 'spread out' the values are.\n",
    "\n",
    "If we want to apply multiple aggregation functions, then it is a little clearer to first gather the functions in a [list](extras/glossary.ipynb) [variable](extras/glossary.ipynb#variable), and then pass this list variable into the `aggregate()` method. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "summary_stats = [min, statistics.median, statistics.mean, statistics.stdev, max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note an important subtlety here: So far, we have almost always placed parentheses after function names, in order to [call](extras/glossary.ipynb#call) them immediately and get their [return value](extras/glossary.ipynb#return). However, this is not the right thing to do here. If we are merely referring to a function, for example to store it in a list or to pass it in as the input [argument](extras/glossary.ipynb#argument) to another function, then we do not want to call the function yet, so no parentheses.\n",
    "\n",
    "Now we can give our list of summary functions to the `aggregate()` method of our `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Bird   HeartRate      Depth   Duration\n",
      "min         EP19   22.800000    5.00000   1.050000\n",
      "median    EP3505   48.400000   40.87000   8.100000\n",
      "max     EP432001  134.000000  225.00000  18.150000\n",
      "mean         NaN   56.924000   56.79936   7.311231\n",
      "stdev        NaN   26.343033   47.75013   4.074564\n"
     ]
    }
   ],
   "source": [
    "print(penguins.aggregate(summary_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very convenient. Instead of doing all the math ourselves for each column (and risking getting it wrong), with just a couple of commands we get a nice table of mathematical summaries of our data. For example, we can see at a glance that the penguins dived (dove?) up to more than 200 meters, but that an 'average' dive was quite a lot shallower, at about 40 to 50 meters deep.\n",
    "\n",
    "There is one small blemish to fix in our summary. `pandas` has dutifully applied the summary functions to every column, but there is one column for which they do not make any sense. The 'Bird' column contains [strings](extras/glossary.ipynb#string) rather than numbers, so mathematical functions do not apply here. As a result, we see that some of the results for the 'Bird' column are marked '[NaN](extras/glossary.ipynb#nan)'. This is an abbreviation of 'Not a Number', sometimes used in data analysis to represent values that are missing or undefined.\n",
    "\n",
    "Even more strangely, we see that Python seems to think that there is a 'minimum' and 'maximum' bird ID. This is of course nonsensical, and is just an unfortunate side-effect of the fact that Python's `min()` and `max()` functions work by first sorting values, and that string values are sorted alphabetically. So these are simply the alphabetically first and last Bird IDs, which is not a piece of information we need.\n",
    "\n",
    "We can fix these small problems by specifying separate summary functions for each column. `aggregate()` also allows its first [argument](extras/glossary.ipynb#argument) to be a [dictionary](extras/glossary.ipynb#dictionary) (remember those?) in which the [keys](extras/glossary.ipynb#key) are the names of columns, and the value for each key is a list of summary functions for that column only.\n",
    "\n",
    "So we can use a dictionary to summarize only specific columns. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Depth   Duration\n",
      "min       5.00000   1.050000\n",
      "median   40.87000   8.100000\n",
      "mean     56.79936   7.311231\n",
      "stdev    47.75013   4.074564\n",
      "max     225.00000  18.150000\n"
     ]
    }
   ],
   "source": [
    "summary_dict = {'Depth': summary_stats,\n",
    "                'Duration': summary_stats}\n",
    "\n",
    "print(penguins.aggregate(summary_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can customize the summary for each column. For example to get shorter summaries of some columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HeartRate  Depth  Duration   Bird\n",
      "count          NaN    NaN       NaN  125.0\n",
      "max     134.000000  225.0     18.15    NaN\n",
      "mean     56.924000    NaN       NaN    NaN\n",
      "median   48.400000    NaN       NaN    NaN\n",
      "min      22.800000    5.0      1.05    NaN\n",
      "stdev    26.343033    NaN       NaN    NaN\n"
     ]
    }
   ],
   "source": [
    "summary_dict = {'HeartRate': summary_stats,\n",
    "                'Depth': [min, max],\n",
    "                'Duration': [min, max]}\n",
    "\n",
    "print(penguins.aggregate(summary_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`aggregate()` also has an alternative abbreviated name (or 'alias'): `agg()`. You can use this instead if you're into the whole, like, brevity thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HeartRate  Depth  Duration   Bird\n",
      "count          NaN    NaN       NaN  125.0\n",
      "max     134.000000  225.0     18.15    NaN\n",
      "mean     56.924000    NaN       NaN    NaN\n",
      "median   48.400000    NaN       NaN    NaN\n",
      "min      22.800000    5.0      1.05    NaN\n",
      "stdev    26.343033    NaN       NaN    NaN\n"
     ]
    }
   ],
   "source": [
    "print(penguins.agg(summary_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same and it doesn't matter which you use. Just be consistent.\n",
    "\n",
    "#### Categories\n",
    "\n",
    "The 'Bird' column in our penguins data frame represents categories of observations, where those categories have no natural order and cannot usefully be thought of as quantities on a scale. As we saw above, this makes mathematical summaries unsuitable. But we can still summarize categorical columns of data, and indeed it is usually a good idea to do so.\n",
    "\n",
    "The main thing that we want to know about a categorical column is how many observations there are in each category. In our example, this is asking how many dives were recorded for each penguin. The `count()` [method](extras/glossary.ipynb#method), applied to a single column, counts the number of times each value occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP19        20\n",
      "EP432001    17\n",
      "EP3205      17\n",
      "EP3505      16\n",
      "EP3901      14\n",
      "EP3905      13\n",
      "EP22        12\n",
      "EP3105      10\n",
      "EP3605       6\n",
      "Name: Bird, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(penguins['Bird'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see for example that we have only 6 dives from penguin 'EP3605', and at least 10 dives from the other penguins.\n",
    "\n",
    "If we would like instead to see how many observations we have in each category, as a proportion of the total number of observations, we can set the `normalize` [argument](extras/glossary.ipynb#argument) to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP19        0.160\n",
      "EP432001    0.136\n",
      "EP3205      0.136\n",
      "EP3505      0.128\n",
      "EP3901      0.112\n",
      "EP3905      0.104\n",
      "EP22        0.096\n",
      "EP3105      0.080\n",
      "EP3605      0.048\n",
      "Name: Bird, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(penguins['Bird'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportions can sometimes be useful for checking whether our data are representative of the population in general with respect to some demographic characteristic. For example in market research, it might be important to have representative proportions of customers of different genders, religions, pet preferences, etc.\n",
    "\n",
    "#### Grouping\n",
    "\n",
    "Above we produced a summary of the depths and durations of dives. What if we wanted to have this information separately for each penguin? We might want to know whether the penguins all behaved more or less the same or whether they have very varied 'diving strategies'. For this we need a 'grouped summary'.\n",
    "\n",
    "`pandas` makes grouping our data pretty easy. The `groupby()` [method](extras/glossary.ipynb#method) groups the rows of a data frame according to the categories in one or more columns. The first [argument](extras/glossary.ipynb#argument) is a list of column names to group by. Any subsequent calculations are then performed separately for the groups of observations that are in the same category for those columns. The most common pattern is to apply the `groupby()` method and then immediately apply the `aggregate()` method to the result. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HeartRate         Depth          Duration           \n",
      "               min    max    min     max       min        max\n",
      "Bird                                                         \n",
      "EP19          31.8  134.0   5.00   45.50  1.050000  11.866667\n",
      "EP22          24.1  106.3  11.00  147.00  2.000000  18.150000\n",
      "EP3105        27.5   81.3  20.41   75.97  1.933317  14.149933\n",
      "EP3205        29.6  104.5  19.24   66.94  1.100000  15.449917\n",
      "EP3505        22.8  121.7  10.89   63.82  1.100000  14.683250\n",
      "EP3605        23.5   73.3  22.75   67.38  2.183333  12.399933\n",
      "EP3901        42.1  127.8   8.60  225.00  1.533333  13.533333\n",
      "EP3905        31.3   84.4  12.30   66.45  1.050000  12.249933\n",
      "EP432001      34.0  103.8  25.00  170.00  1.916667  10.500000\n"
     ]
    }
   ],
   "source": [
    "print(penguins.groupby(['Bird']).aggregate([min, max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here this shows us that penguin 'EP3901' has an impressively wide range of dive depths, and generally a faster heart rate. This is perhaps a younger energetic penguin (though I don't know enough about penguin ethology to say).\n",
    "\n",
    "In other contexts, grouped summaries are useful for comparing things like types of customer, countries, companies, etc. In a basic data analysis, we will almost always want a grouped summary at some point.\n",
    "\n",
    "## Data visualization"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
